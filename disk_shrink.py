#!/usr/bin/env python3
import os
import hashlib
from pathlib import Path
import argparse
from collections import defaultdict

import shutil
import subprocess
from datetime import datetime

def _format_bytes(size):
    """æ ¼å¼åŒ–å­—èŠ‚æ˜¾ç¤º"""
    units = ('B', 'KB', 'MB', 'GB')
    index = 0
    while size >= 1024 and index < 3:
        size /= 1024
        index += 1
    return f"{size:.2f} {units[index]}"

class DiskAnalyzer:
    def __init__(self, target_path, chunk_size=8192, quiet_mode=False):
        self.target_path = Path(target_path).expanduser()
        self.file_stats = defaultdict(list)
        self.chunk_size = chunk_size
        self.total_saved = 0
        self.quiet_mode = quiet_mode  # æ–°å¢é™é»˜æ¨¡å¼å¼€å…³

    def analyze_filesystem(self):
        """ä¸»åˆ†æå‡½æ•°"""
        if not self.quiet_mode:
            print("\nğŸ” å¼€å§‹å®æ—¶æ–‡ä»¶åˆ†æ...")
        self._recover_orphaned_files()
        
        for file_path in self.target_path.rglob('*'):
            # æ–°å¢ç¬¦å·é“¾æ¥æ£€æŸ¥
            if file_path.is_symlink():
                print(f"â© è·³è¿‡ç¬¦å·é“¾æ¥ï¼š{file_path}")
                continue
                
            if file_path.is_file():
                self._collect_file_metadata(file_path)
        
        duplicates = self._find_duplicate_files()
        
        return {
            'total_files': sum(len(v) for v in self.file_stats.values()),
            'estimated_saved': self.total_saved,
            'duplicates': duplicates
        }

    def _calculate_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆSHA-256ï¼‰"""
        hasher = hashlib.sha256()
        with file_path.open('rb') as f:
            while chunk := f.read(self.chunk_size):
                hasher.update(chunk)
        return hasher.hexdigest()

    def _compare_files(self, file1, file2):
        """äºŒè¿›åˆ¶å†…å®¹æ¯”å¯¹"""
        with file1.open('rb') as f1, file2.open('rb') as f2:
            while True:
                b1 = f1.read(self.chunk_size)
                b2 = f2.read(self.chunk_size)
                if b1 != b2:
                    return False
                if not b1:
                    return True

    def _collect_file_metadata(self, file_path):
        # æ–°å¢äºŒæ¬¡éªŒè¯ï¼ˆé˜²æ­¢é€šè¿‡ç¬¦å·é“¾æ¥è®¿é—®æ–‡ä»¶ï¼‰
        if file_path.is_symlink():
            return
            
        stat = file_path.stat()
        file_size = stat.st_size
        
        if file_size == 0:
            return
        
        file_hash = self._calculate_file_hash(file_path)
        file_size = stat.st_size
        
        existing_files = self.file_stats.get(file_hash, [])
        if existing_files:
            if not self.quiet_mode:  # é™é»˜æ¨¡å¼ä¸‹ä¸è¾“å‡ºé‡å¤æ–‡ä»¶ä¿¡æ¯
                print(f"ğŸ” å‘ç°é‡å¤æ–‡ä»¶ï¼š{file_path}")
                print(f"  åŸå§‹æ–‡ä»¶ï¼š{existing_files[0]['path']}")
                print(f"  é¢„ä¼°èŠ‚çœç©ºé—´ï¼š{_format_bytes(file_size)}\n")
            self.total_saved += file_size

        self.file_stats[file_hash].append({
            'path': str(file_path),
            'size': file_size,
            'created': stat.st_ctime,
            'modified': stat.st_mtime,
            'file_type': file_path.suffix.lower()
        })

    def _find_duplicate_files(self):
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        return {
            h: files for h, files in self.file_stats.items()
            if len(files) > 1
        }

    def deduplicate_files(self):
        import uuid
        
        for file_hash, files in self.file_stats.items():
            if len(files) > 1:
                files.sort(key=lambda x: x['created'])
                retained = files[0]
                
                for f in files[1:]:
                    original_path = f['path']
                    current_file = Path(original_path)
                    
                    if not current_file.exists():
                        print(f"â© è·³è¿‡å·²åˆ é™¤æ–‡ä»¶ï¼š{original_path}")
                        continue

                    temp_path = f"{original_path}.{uuid.uuid4().hex}.bak"
                    
                    try:
                        print(f"âŒ› å¼€å§‹å¤„ç†ï¼š{original_path}")
                        
                        # ä¿å­˜åŸå§‹å…ƒæ•°æ®
                        current_stat = current_file.stat()
                        
                        Path(original_path).rename(temp_path)
                        subprocess.run(['cp', '-c', retained['path'], original_path], check=True)
                        
                        # æ¢å¤å…ƒæ•°æ®
                        os.utime(original_path, (current_stat.st_atime, current_stat.st_mtime))

                        if not self._compare_files(Path(retained['path']), Path(original_path)):
                            raise RuntimeError("æ–‡ä»¶éªŒè¯å¤±è´¥")
                            
                        Path(temp_path).unlink()
                        print(f"âœ… å®Œæˆå¤„ç†ï¼š{original_path}\n")
                        
                    except Exception as e:
                        if Path(temp_path).exists() and not Path(original_path).exists():
                            Path(temp_path).rename(original_path)
                        print(f"âŒ å¤„ç†å¤±è´¥ï¼š{original_path} ({e})\n")

    def _recover_orphaned_files(self):
        import re
        print("\nğŸ” æ‰«ææ®‹ç•™ä¸´æ—¶æ–‡ä»¶...")
        recovered = 0
        
        temp_file_pattern = re.compile(r'^(.+)\.([0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12})\.bak$')
        
        for temp_file in self.target_path.rglob('*'):
            if match := temp_file_pattern.match(temp_file.name):
                original_name = match.group(1)
                uuid_part = match.group(2)
                original_path = temp_file.parent / original_name
                
                if not self._is_valid_uuid(uuid_part):
                    print(f"âš ï¸ å¿½ç•¥æ— æ•ˆUUIDæ ¼å¼ï¼š{temp_file}")
                    continue
                    
                if original_path.exists():
                    if self._compare_files(original_path, temp_file):
                        temp_file.unlink()
                        print(f"âœ… æ¸…ç†å·²å®Œæˆçš„ä¸´æ—¶æ–‡ä»¶ï¼š{temp_file}")
                        recovered +=1
                    else:
                        try:
                            original_path.unlink()
                            temp_file.rename(original_path)
                            recovered +=1
                            print(f"ğŸ”„ æ¢å¤ä¸ä¸€è‡´æ–‡ä»¶ï¼š{temp_file} -> {original_path}")
                        except Exception as e:
                            print(f"âŒ æ¢å¤å¤±è´¥ï¼š{temp_file} -> {original_path} ({e})")
                else:
                    try:
                        temp_file.rename(original_path)
                        print(f"ğŸ”„ æ¢å¤æœªå®Œæˆæ“ä½œï¼š{original_path}")
                        recovered +=1
                    except Exception as e:
                        print(f"âŒ æ¢å¤å¤±è´¥ï¼š{temp_file} -> {original_path} ({e})")
        
        print(f"ä¸´æ—¶æ–‡ä»¶æ¢å¤å®Œæˆï¼Œå…±å¤„ç†{recovered}ä¸ªæ–‡ä»¶\n")

    def _is_valid_uuid(self, uuid_str):
        """éªŒè¯æ˜¯å¦ä¸ºæ ‡å‡†UUIDçš„hexæ ¼å¼"""
        import uuid
        try:
            uuid.UUID(hex=uuid_str, version=4)
            return True
        except ValueError:
            return False

        # ç”Ÿæˆå”¯ä¸€ä¸´æ—¶æ–‡ä»¶åï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        temp_path = f"{original_path}.{uuid.uuid4().hex}.bak"

def _check_cow_support(target_path):
    """æ£€æŸ¥æ˜¯å¦æ»¡è¶³COWæ‰§è¡Œæ¡ä»¶"""
    target_path = Path(target_path).resolve()
    
    if not target_path.exists():
        raise SystemExit(f"âŒ é”™è¯¯ï¼šè·¯å¾„ä¸å­˜åœ¨ {target_path}")

    # æ–°å¢è·å–çœŸå®æŒ‚è½½ç‚¹
    try:
        df_output = subprocess.check_output(
            ['df', str(target_path)], 
            stderr=subprocess.STDOUT,
            text=True
        ).splitlines()[1].split()[0]
    except Exception as e:
        raise SystemExit(f"âŒ æŒ‚è½½ç‚¹è·å–å¤±è´¥: {e}")

    try:
        output = subprocess.check_output(
            ['diskutil', 'info', df_output],
            stderr=subprocess.STDOUT,
            text=True
        )
        if 'APFS' not in output:
            raise SystemExit(f"âŒ é”™è¯¯ï¼šç›®æ ‡ç›®å½•å¿…é¡»ä½äºAPFSæ–‡ä»¶ç³»ç»Ÿï¼ˆå½“å‰æ–‡ä»¶ç³»ç»Ÿï¼š{output.split('File System Personality: ')[-1].split()[0]}ï¼‰")
    except subprocess.CalledProcessError as e:
        raise SystemExit(f"âŒ æ–‡ä»¶ç³»ç»Ÿæ£€æµ‹å¤±è´¥: {e.output}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='macOSç£ç›˜æ–‡ä»¶åˆ†æå·¥å…·')
    parser.add_argument('path', help='è¦åˆ†æçš„ç›®å½•è·¯å¾„')
    parser.add_argument('--shrink', action='store_true', 
                       help='æ‰§è¡Œç©ºé—´æ”¶ç¼©ï¼ˆè‡ªåŠ¨å»é‡+COWæ¢å¤ï¼‰')
    args = parser.parse_args()

    # æ‰§è¡Œå‰ç½®æ£€æŸ¥
    if args.shrink:
        print("ğŸ” æ­£åœ¨æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ...")
        _check_cow_support(args.path)
        print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼ˆmacOS + APFSï¼‰\n")

    analyzer = DiskAnalyzer(args.path, quiet_mode=args.shrink)
    report = analyzer.analyze_filesystem()
    
    # å½“ä¸æŒ‡å®šå‚æ•°æ—¶æ˜¾ç¤ºåˆ†ææŠ¥å‘Š
    if not args.shrink:
        print(f"\nåˆ†æå®Œæˆï¼š{args.path}")
        print(f"æ€»æ–‡ä»¶æ•°: {report['total_files']}")
        print(f"å‘ç°é‡å¤æ–‡ä»¶ç»„: {len(report['duplicates'])} ç»„")
        print(f"é¢„ä¼°å¯èŠ‚çœç©ºé—´: {_format_bytes(report['estimated_saved'])}\n")
    
    # åˆå¹¶å»é‡å’Œæ¢å¤æ“ä½œä¸ºä¸€ä¸ªå‚æ•°
    if args.shrink:
        print("æ­£åœ¨æ‰§è¡Œå®æ—¶ç©ºé—´ä¼˜åŒ–...")
        # æ–°å¢ç©ºé—´ç»Ÿè®¡
        before_usage = shutil.disk_usage(args.path)
        analyzer.deduplicate_files()
        after_usage = shutil.disk_usage(args.path)
        saved_space = before_usage.used - after_usage.used
        
        # æ–°å¢ç©ºé—´ç»Ÿè®¡è¾“å‡º
        print("\nç©ºé—´ä¼˜åŒ–ç»Ÿè®¡:")
        print(f"åˆå§‹å ç”¨ç©ºé—´: {_format_bytes(before_usage.used)}")
        print(f"ä¼˜åŒ–åå ç”¨ç©ºé—´: {_format_bytes(after_usage.used)}")
        print(f"èŠ‚çœå­˜å‚¨ç©ºé—´: {_format_bytes(saved_space)}")
        print("ç©ºé—´ä¼˜åŒ–å®Œæˆï¼Œå·²é€šè¿‡APFSå…‹éš†æ›¿æ¢æ‰€æœ‰é‡å¤æ–‡ä»¶")